---
title: "Predictive Analysis"
author: "Sumon Barua"
output:
  html_document:
    keep_md: true

---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Examples were taken from the book: 
  
  * Graphics Cookbook: Practical Recipes for Visualizing Data
  * Cookbook for R


```{r}
#install.packages("tidyverse")
#install.packages("gridExtra")
library(tidyverse)
library(gridExtra)
```


####Correlation Analysis

* Level of linear dependence two variables
* Range of correlation coefficient -> -1 to +1
* Perfect positive relationship    -> +1
* Perfect Negative relationship    -> -1
* No linear relationship           -> 0
* Good relationship                -> >+-0.85
* Correlation is positive when the values increase together
* Correlation is negative when one decreases when other value increases
* Scatter plot helps to identify correlation visually
* Correlation works well when the relationship is a straight line
* Sometimes calculation may not pick up relationship but visuals can
* Correlation is not causation - Correlation does not mean that one thing causes other.

```{r}
diamonds

ggplot(diamonds, aes(x=carat, y=price)) +
  geom_point()

```

####Scatter Plot

* Good relationship



####Simple linear regression / univariate regression

* This identifies linear relationship between predictor/independent and response/dependent.
* This predicts response variable based on the independent variable.
* An extension of correlation.
* One response variable and a single independent variable
* Best fitting straight line for a scatter plot between two variables
* The function lm fits a linear model to data
* Coefficient measures the slope of the relationship
* A type of supervised statistical learning approach that is useful for predicting a quantitative response Y
* intercept + slope * Independent variable + error term
* intercept and slope are also called beta coefficient
* Scatter plot :Indicates the relationship between variables
* Boxplot plot :To identify outlier
* Density plot :Distribustion of independent variable
* Hypothesis : p-Value is associated with Null and alternate hypothesis
* Null hypothesis: this is the initial hypothesis assuming there is no relationship (associated coefficient is equal to zero)
* It is very important for the model to be statistically significant before we decide to use it
* p-Value < pre-determined level(0.05) indicates that the model is statistically significance and we can reject Null hypothesis
* More stars are next to p-Value means more statistically signigicant
* Higher the t-value, the better
* tilde(~) indicates "depends on"
* Residual = Observed - Predicted
* The most useful way to plot the residuals, though, is with your predicted values on the x-axis, and your residuals on the y-axis. The distance from the line at 0 is how bad the prediction was for that value


```{r}
head(cars)
str(cars)


p1 <- ggplot(cars, aes(x=speed, y=dist)) +
  geom_point()

p2 <- ggplot(cars, aes(x=speed, y=dist)) +
  geom_point() +
  geom_smooth()

grid.arrange(p1, p2, ncol=2)


#normality check
par(mfrow=c(1,2))
qqnorm(cars$speed)
qqline(cars$speed)
qqnorm(cars$dist)
qqline(cars$dist)

#outliers
par(mfrow=c(1,2))
boxplot(cars$speed, main="speed")
boxplot(cars$dist, main="dist")


#check normal distribution
p1 <- ggplot(data = NULL, aes(x = cars$speed)) +
        geom_histogram(aes(y = ..density..), colour="black", fill="white") +
        geom_density(alpha=.2, fill="#FF6666") +
        geom_vline(aes(xintercept=mean(cars$speed, na.rm = T)), color="red", linetype="dashed", size=1)

p2 <- ggplot(data = NULL, aes(x = cars$dist)) +
        geom_histogram(aes(y = ..density..), colour="black", fill="white") +
        geom_density(alpha=.2, fill="#FF6666") +
        geom_vline(aes(xintercept=mean(cars$dist, na.rm = T)), color="red", linetype="dashed", size=1)

grid.arrange(p1, p2, ncol=2)

#correlation
cor(cars$speed, cars$dist)

#Simple linear model
#response~independent
model_simplelm <- lm(dist~speed, data = cars)
summary(model_simplelm)

```

####Predicting Simple linear model

* Split data into training and test set
* Training data : 80% of the data
* Test data : Remaining 20% of the data
* Build model based on traing data
* Now predict test data using above model

```{r}
ds <- cars

set.seed(123)
#split training and sample data
ds.training.index <- sample(1:nrow(cars), 0.8*nrow(cars))
ds.training <- ds[ds.training.index,]
ds.test <- ds[-ds.training.index,]

#buil model based on training data
ds.training.fit <- lm(dist~speed, data = ds.training)
#test data prediction
ds.test$dist_predicted <- predict(ds.training.fit, ds.test)

#display result
ds.test


plot(ds.test$speed, ds.test$dist)
abline(ds.training.fit)

ds.training.fit$coefficients
ds.training.fit$coefficients[1] #intercept
ds.training.fit$coefficients[2] #slope

ggplot(ds.test, aes(x = speed, y = dist)) +
  geom_point() +
  geom_abline(intercept = ds.training.fit$coefficients[1], slope = ds.training.fit$coefficients[2])

#only with ggplot2
ggplot(cars, aes(x = speed, y = dist)) +
  geom_point() +
  geom_smooth(method = lm)

```
